{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv('../Data/raw_data_b/user_register_log.txt', header=None,names=['user_id','register_day','register_type','device_type'],sep='\\t') # 注册用户数据加载\n",
    "user_login = pd.read_csv('../Data/raw_data_b/app_launch_log.txt', header=None,names=['user_id','day'],sep='\\t') # app登录日志数据加载\n",
    "user_act = pd.read_csv('../Data/raw_data_b/user_activity_log.txt', header=None,names=['user_id','day','page','video_id','author_id','action_type'],sep='\\t') # 用户行为日志数据加载\n",
    "user_video = pd.read_csv('../Data/raw_data_b/video_create_log.txt', header=None,names=['user_id','day'],sep='\\t') # 用户拍摄视频日志数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有数据都是1-30天内的数据\n",
    "\n",
    "且无缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据进行滑窗法划分："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: \n",
    "\n",
    "    +-data1\n",
    "        --feature:1-15;label:16-22\n",
    "    +-data2\n",
    "        --feature:9-23;label:24-30\n",
    "    +-data3\n",
    "        --feature:1-30;label: 31-37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cutDataFunc(data, cut_col ,start_day, end_day):\n",
    "    return data[(data[cut_col]<=end_day)&(data[cut_col]>=start_day)]\n",
    "\n",
    "def cutDataByTime(data_url, start_day, end_day):\n",
    "    temp_users = cutDataFunc(users, 'register_day', start_day, end_day)\n",
    "    temp_login = cutDataFunc(user_login, 'day', start_day, end_day)\n",
    "    temp_act = cutDataFunc(user_act, 'day', start_day, end_day)\n",
    "    temp_video = cutDataFunc(user_video, 'day', start_day, end_day)\n",
    "    \n",
    "    temp_users.to_csv(data_url+'users.csv',index=False)\n",
    "    temp_login.to_csv(data_url+'login.csv',index=False)\n",
    "    temp_act.to_csv(data_url+'act.csv',index=False)\n",
    "    temp_video.to_csv(data_url+'video.csv',index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutDataProgram():\n",
    "    start = time.time()\n",
    "    print (\"---------START-----------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data1/train_', 1, 15)\n",
    "    cutDataByTime('../Data/data1/test_', 16, 22)\n",
    "    print (\"-----第1数据集完成-------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data2/train_', 9, 23)\n",
    "    cutDataByTime('../Data/data2/test_', 24, 30)\n",
    "    print (\"-----第2数据集完成-------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data3/train_', 1, 30)\n",
    "    print (\"-----第3数据集完成-------\")\n",
    "    \n",
    "    print (\"----------END------------\")\n",
    "    end = time.time()\n",
    "    print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------START-----------\n",
      "-----第1数据集完成-------\n",
      "-----第2数据集完成-------\n",
      "-----第3数据集完成-------\n",
      "----------END------------\n",
      "238.051616191864\n"
     ]
    }
   ],
   "source": [
    "cutDataProgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train集构造和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取有活跃行为的用户集，（从test集中获取）\n",
    "def getActivityUsers(data_url):\n",
    "    test_login = pd.read_csv(data_url+'test_login.csv')\n",
    "    test_act = pd.read_csv(data_url+'test_act.csv')\n",
    "    test_video = pd.read_csv(data_url+'test_video.csv')\n",
    "    \n",
    "    activity_user = np.unique(pd.concat([test_login['user_id'], test_act['user_id'], test_video['user_id']]))\n",
    "    return activity_user\n",
    "\n",
    "def get_diff_from_ls(x):\n",
    "    x.sort()\n",
    "    return list(np.diff(x))\n",
    "\n",
    "# （video and lanuch）data create feature method\n",
    "# 1、将day转换成距离时间窗口截点的距离\n",
    "# 2、描述性day、cnt统计特征\n",
    "# 3、连续1、 2、 3、 7天内的统计特征\n",
    "def getCountFeature(data, name='login'):\n",
    "    day_max = max(data['day'])\n",
    "    data['day'] = day_max - data.day\n",
    "    df = data.groupby(['user_id','day'],as_index=False).apply(lambda x:x.shape[0])\n",
    "    df_temp = pd.DataFrame(df, columns=['cnt']).reset_index()\n",
    "    \n",
    "    df_temp_group = df_temp.groupby(['user_id'],as_index=False)\n",
    "    res_df = df_temp_group.agg({'day':['max','min','std'],'cnt':['count','sum','max','var','mean']})\n",
    "    res_df.columns = ['user_id', name+'_day_max',name+'_day_min',name+'_day_std',name+'_cnt',name+'_sum',name+'_max',name+'_var',name+'_mean']\n",
    "    \n",
    "    # 日期差分统计特征\n",
    "#     df_temp_diff = df_temp[['user_id','day']].groupby('user_id').aggregate(lambda x: list(set(x)))\n",
    "#     df_temp_diff.day = df_temp_diff.day.apply(lambda x:get_diff_from_ls(x))\n",
    "#     res_df[name+'_day_diff_max'] = df_temp_diff.day.apply(lambda x: max(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_min'] = df_temp_diff.day.apply(lambda x: min(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_mean'] = df_temp_diff.day.apply(lambda x: np.mean(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_std'] = df_temp_diff.day.apply(lambda x: pd.Series(x).std() if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_skew'] = df_temp_diff.day.apply(lambda x: pd.Series(x).skew()).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_kurt'] = df_temp_diff.day.apply(lambda x: pd.Series(x).kurt()).fillna(-1).values\n",
    "    \n",
    "    \n",
    "    for day_len in [1,3,7]:\n",
    "        df_temp_day = df_temp[(df_temp.day>=0) & (df_temp.day<day_len)]\n",
    "        df_temp_day_cnt = df_temp_day.groupby(['user_id']).apply(lambda x:sum(x['cnt'].values))\n",
    "        add_df = pd.DataFrame(df_temp_day_cnt, columns=[name+'_'+str(day_len)+'_cnt']).reset_index()\n",
    "        res_df = pd.merge(res_df, add_df, on='user_id', how='left').fillna(0)\n",
    "        if day_len != 1:\n",
    "            res_df[name+'_'+str(day_len)+'_cnt_arg'] = res_df[name+'_'+str(day_len)+'_cnt'] / day_len\n",
    "            \n",
    "    res_df['login_cnt_weight_sum'] = 0\n",
    "    if name == 'login':\n",
    "        for day_len in [1,3,7]:\n",
    "            res_df['login_cnt_weight_sum'] += res_df[name+'_'+str(day_len)+'_cnt']*(8-day_len)\n",
    "    return res_df.fillna(0)\n",
    "\n",
    "# act: create feature method\n",
    "# 1、将day转换成距离时间窗口截点的距离\n",
    "# 2、描述性day、cnt统计特征\n",
    "# 3、连续1、 2、 3、 7天内的统计特征\n",
    "# 4、page和action_type不同类型的所有统计数以及比率特征\n",
    "# 5、page和action_type不同类型在连续1、 3、 7天内的统计特征\n",
    "# 6、user_id是否为author_id的成员\n",
    "def getCountFeatureAboutAct(data, name='act'):\n",
    "    day_max = max(data['day'])\n",
    "    data['day'] = day_max - data.day\n",
    "    authors = set(data['author_id'])\n",
    "    \n",
    "    df = data.groupby(['user_id','day'],as_index=False).apply(lambda x:x.shape[0])\n",
    "    df_temp = pd.DataFrame(df, columns=['cnt']).reset_index()\n",
    "    \n",
    "    df_temp_group = df_temp.groupby(['user_id'],as_index=False)\n",
    "    res_df = df_temp_group.agg({'day':['max','min','std'],'cnt':['count','sum','max','var','mean']})\n",
    "    res_df.columns = ['user_id', name+'_day_max',name+'_day_min',name+'_day_std',name+'_cnt',name+'_sum',name+'_max',name+'_var',name+'_mean']\n",
    "    \n",
    "    # 日期差分统计特征\n",
    "#     df_temp_diff = df_temp[['user_id','day']].groupby('user_id').aggregate(lambda x: list(set(x)))\n",
    "#     df_temp_diff.day = df_temp_diff.day.apply(lambda x:get_diff_from_ls(x))\n",
    "#     res_df[name+'_day_diff_max'] = df_temp_diff.day.apply(lambda x: max(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_min'] = df_temp_diff.day.apply(lambda x: min(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_mean'] = df_temp_diff.day.apply(lambda x: np.mean(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_std'] = df_temp_diff.day.apply(lambda x: pd.Series(x).std() if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_skew'] = df_temp_diff.day.apply(lambda x: pd.Series(x).skew()).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_kurt'] = df_temp_diff.day.apply(lambda x: pd.Series(x).kurt()).fillna(-1).values\n",
    "    \n",
    "    for day_len in [1,3,7]:\n",
    "        df_temp_day = df_temp[(df_temp.day>=0) & (df_temp.day<day_len)]\n",
    "        df_temp_day_cnt = df_temp_day.groupby(['user_id']).apply(lambda x:sum(x['cnt'].values))\n",
    "        add_df = pd.DataFrame(df_temp_day_cnt, columns=[name+'_'+str(day_len)+'_cnt']).reset_index()\n",
    "        res_df = pd.merge(res_df, add_df, on='user_id', how='left').fillna(0)\n",
    "        if day_len != 1:\n",
    "            res_df[name+'_'+str(day_len)+'_cnt_arg'] = res_df[name+'_'+str(day_len)+'_cnt'] / day_len\n",
    "    \n",
    "    # page 处理\n",
    "    temp = data.groupby(['user_id','day','page'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "    for day_len in [1,3,7]:\n",
    "        for col in [0,1,2,3,4]:\n",
    "            temp_day = temp[(temp.day>=0) & (temp.day<day_len)]\n",
    "            df_temp_day_cnt = temp_day.groupby(['user_id']).apply(lambda x:sum(x[col].values))\n",
    "            add_df = pd.DataFrame(df_temp_day_cnt, columns=['page_'+str(col)+'_'+str(day_len)+'_cnt']).reset_index()\n",
    "            res_df = pd.merge(res_df, add_df, on='user_id', how='left').fillna(0)\n",
    "            if day_len != 1:\n",
    "                res_df['page_'+str(col)+'_'+str(day_len)+'_cnt_arg'] = res_df['page_'+str(col)+'_'+str(day_len)+'_cnt'] / day_len\n",
    "    \n",
    "    page = data.groupby(['user_id','page'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "    page_sum = page[0]+page[1]+page[2]+page[3]+page[4]\n",
    "    res_df['page_0_sigle'] = page[0] / page_sum\n",
    "    res_df['page_1_sigle'] = page[1] / page_sum\n",
    "    res_df['page_2_sigle'] = page[2] / page_sum\n",
    "    res_df['page_3_sigle'] = page[3] / page_sum\n",
    "    res_df['page_4_sigle'] = page[4] / page_sum\n",
    "#     res_df['page_sum'] = page_sum\n",
    "    res_df[['page_0','page_1','page_2','page_3','page_4']] = page[[0,1,2,3,4]]\n",
    "    \n",
    "    # action_type处理\n",
    "    temp = data.groupby(['user_id','day','action_type'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "    temp_group_action = temp.groupby(['user_id'])\n",
    "    for day_len in [1,3,7]:\n",
    "        for col in [0,1,2,3,4,5]:\n",
    "            temp_day = temp[(temp.day>=0) & (temp.day<day_len)]\n",
    "            df_temp_day_cnt = temp_day.groupby(['user_id']).apply(lambda x:sum(x[col].values))\n",
    "            add_df = pd.DataFrame(df_temp_day_cnt, columns=['action_type_'+str(col)+'_'+str(day_len)+'_cnt']).reset_index()\n",
    "            res_df = pd.merge(res_df, add_df, on='user_id', how='left').fillna(0)\n",
    "            if day_len != 1:\n",
    "                res_df['action_type_'+str(col)+'_'+str(day_len)+'_cnt_arg'] = res_df['action_type_'+str(col)+'_'+str(day_len)+'_cnt'] / day_len\n",
    "    \n",
    "    action_type = data.groupby(['user_id','action_type'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "    action_type_sum = action_type[0]+action_type[1]+action_type[2]+action_type[3]+action_type[4]+action_type[5]\n",
    "    res_df['action_type_0_sigle'] = action_type[0] / action_type_sum\n",
    "    res_df['action_type_1_sigle'] = action_type[1] / action_type_sum\n",
    "    res_df['action_type_2_sigle'] = action_type[2] / action_type_sum\n",
    "    res_df['action_type_3_sigle'] = action_type[3] / action_type_sum\n",
    "    res_df['action_type_4_sigle'] = action_type[4] / action_type_sum\n",
    "    res_df['action_type_5_sigle'] = action_type[5] / action_type_sum\n",
    "    #     res_df['action_type_sum'] = action_type_sum\n",
    "    res_df[['action_type_0','action_type_1','action_type_2','action_type_3','action_type_4','action_type_5']] = action_type[[0,1,2,3,4,5]]\n",
    "    \n",
    "    res_df['is_author'] = res_df['user_id'].apply(lambda x: 1 if x in authors else 0)\n",
    "    \n",
    "    return res_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取最近一天的点击数量\n",
    "def getCntOfOneDay(data):\n",
    "    return sum(data[(data.day==0)]['cnt'].values)\n",
    "\n",
    "# 获取最近一段时间内的点击数量\n",
    "def getCntOfSomeDay(data, day_len=7):\n",
    "    return sum(data[(data.day>=0) & (data.day<day_len)]['cnt'].values)\n",
    "\n",
    "# 获取最近一段时间内的点击数量（改进版，可以自由选择col统计）\n",
    "def getCntOfSomeDayWithCol(data, day_len=1, col='page'):\n",
    "    return sum(data[(data.day>=0) & (data.day<day_len)][col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过video 、 act 、 register 、 lauch来构造特征\n",
    "def constructDataFeature(data_url):\n",
    "    train_login = pd.read_csv(data_url+'train_login.csv')\n",
    "    train_act = pd.read_csv(data_url+'train_act.csv')\n",
    "    train_video = pd.read_csv(data_url+'train_video.csv')\n",
    "    \n",
    "    # register data\n",
    "    train_user = pd.read_csv(data_url+'train_users.csv')\n",
    "    feature = train_user\n",
    "    max_day = max(feature['register_day'])\n",
    "    feature['register_day'] = max_day - feature.register_day  # day改成时间窗口截点距离\n",
    "#     feature['register_day_diff'] = max_day - feature.register_day  # day改成时间窗口截点距离\n",
    "    \n",
    "    # login data\n",
    "    train_login_feas = getCountFeature(train_login, 'login')\n",
    "    feature = pd.merge(feature, train_login_feas, on='user_id', how='left')\n",
    "    \n",
    "    # video data\n",
    "    train_video_feas = getCountFeature(train_video, 'video')\n",
    "    feature = pd.merge(feature, train_video_feas, on='user_id', how='left')\n",
    "    \n",
    "    # act data\n",
    "    train_act_feas = getCountFeatureAboutAct(train_act, 'act')\n",
    "    feature = pd.merge(feature, train_act_feas, on='user_id', how='left')\n",
    "    \n",
    "#     feature[ [c for c in feature.columns if 'day_diff' in c ] ] = feature[ [c for c in feature.columns if 'day_diff' in c ] ].fillna(-1)\n",
    "    \n",
    "    # 缺失值全部补NAN\n",
    "    return feature\n",
    "\n",
    "# 通过TEST未来几天内的活跃用户来给Train集标签\n",
    "def getTrainLabel(data_url, data):\n",
    "    # get activity label of train from test_dataset\n",
    "    train_label = []\n",
    "    activity_users = getActivityUsers(data_url)\n",
    "    for u in data['user_id']:\n",
    "        if u in activity_users:\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "    data['label'] = train_label\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataProgram():\n",
    "    start = time.time()\n",
    "    print(\"----------------构造训练集-------------------\")\n",
    "    \n",
    "    data_url = '../Data/data1/'\n",
    "    data1 = constructDataFeature(data_url)\n",
    "    data1 = getTrainLabel(data_url, data1)\n",
    "    print(\"---------第一组数据集处理完成----------------\")\n",
    "    data1.to_csv(data_url+'data1.csv',index=False)\n",
    "    end = time.time()\n",
    "    print (end - start)\n",
    "    \n",
    "    data_url = '../Data/data2/'\n",
    "    data2 = constructDataFeature(data_url)\n",
    "    data2 = getTrainLabel(data_url, data2)\n",
    "    print(\"---------第二组数据集处理完成----------------\")\n",
    "    data2.to_csv(data_url+'data2.csv',index=False)\n",
    "    end = time.time()\n",
    "    print (end - start)\n",
    "    \n",
    "    data_url = '../Data/data3/'\n",
    "    data3 = constructDataFeature(data_url)\n",
    "    print(\"---------第三组数据集处理完成----------------\")\n",
    "    data3.to_csv(data_url+'data3.csv',index=False)\n",
    "    \n",
    "    print(\"--------------------END----------------------\")\n",
    "    end = time.time()\n",
    "    print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------构造训练集-------------------\n",
      "---------第一组数据集处理完成----------------\n",
      "47.5337188243866\n",
      "---------第二组数据集处理完成----------------\n",
      "127.23727750778198\n",
      "---------第三组数据集处理完成----------------\n",
      "--------------------END----------------------\n",
      "237.9376094341278\n"
     ]
    }
   ],
   "source": [
    "getDataProgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型并进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_in = pd.read_csv('../Data/data1/data1.csv')\n",
    "data2_in = pd.read_csv('../Data/data2/data2.csv')\n",
    "data3_in = pd.read_csv('../Data/data3/data3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge feature function\n",
    "def mergeFeatureToData(path, data1_in, data2_in, data3_in, col='user_id'):\n",
    "    feature_data1 = pd.read_csv(path + 'data1.csv')\n",
    "    feature_data2 = pd.read_csv(path + 'data2.csv')\n",
    "    feature_data3 = pd.read_csv(path + 'data3.csv')\n",
    "    \n",
    "    data1_in = pd.merge(data1_in, feature_data1, on=col, how='left')\n",
    "    data2_in = pd.merge(data2_in, feature_data2, on=col, how='left')\n",
    "    data3_in = pd.merge(data3_in, feature_data3, on=col, how='left')\n",
    "    return data1_in, data2_in, data3_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22382, 124)\n",
      "(22382, 144)\n"
     ]
    }
   ],
   "source": [
    "print (data1_in.shape)\n",
    "\n",
    "path = '../Data/cnt_diff_data/'\n",
    "data1_in, data2_in, data3_in = mergeFeatureToData(path, data1_in, data2_in, data3_in)\n",
    "# path = '../Data/day_diff_data/'\n",
    "# data1_in, data2_in, data3_in = mergeFeatureToData(path, data1_in, data2_in, data3_in)\n",
    "path = '../Data/a_va_set_cnt_data/'\n",
    "data1_in, data2_in, data3_in = mergeFeatureToData(path, data1_in, data2_in, data3_in)\n",
    "# path = '../Data/retention_data/'\n",
    "# data1_in, data2_in, data3_in = mergeFeatureToData(path, data1_in, data2_in, data3_in, col='register_day')\n",
    "\n",
    "print (data1_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data1_in.to_csv('..\\Data\\out_data\\data1.csv',index=False)\n",
    "# data2_in.to_csv('..\\Data\\out_data\\data2.csv',index=False)\n",
    "# data3_in.to_csv('..\\Data\\out_data\\data3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Label Encoder Of device\n",
    "cates = pd.Categorical(data3_in['device_type'])\n",
    "categories = cates.categories\n",
    "data3_in['device_type'] = cates.codes\n",
    "data1_in['device_type'] = data1_in['device_type'].apply(lambda x:categories.get_loc(x))\n",
    "data2_in['device_type'] = data2_in['device_type'].apply(lambda x:categories.get_loc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchDaKa():\n",
    "    user_act = pd.read_csv('../Data/raw_data/user_activity_log.txt', header=None,names=['user_id','day','page','video_id','author_id','action_type'],sep='\\t') # 用户行为日志数据加载\n",
    "    authors = user_act.groupby(['author_id']).apply(lambda x:x.shape[0])\n",
    "    author_df = authors.reset_index()\n",
    "    return author_df[author_df[0]>1204]['author_id'].values\n",
    "\n",
    "daka = searchDaKa()\n",
    "data1_in['daka'] = data1_in['user_id'].apply(lambda x: 1 if x in daka else 0)\n",
    "data2_in['daka'] = data2_in['user_id'].apply(lambda x: 1 if x in daka else 0)\n",
    "data3_in['daka'] = data3_in['user_id'].apply(lambda x: 1 if x in daka else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 列举所有的columns\n",
    "for i in range(1,int(len(data2_in.columns)/5)+2):\n",
    "    print (list(data2_in.columns)[(i-1)*5: i*5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 手动标定不需要的cols\n",
    "drop_cols = ['login_sum','login_max','login_var','login_mean','login_3_cnt',\n",
    "             'login_2_cnt','login_7_cnt',\n",
    "             'device_map',\n",
    "             'page_sum', 'action_type_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data1 = data1_in[[c for c in data1_in.columns if c not in drop_cols and c in select_cols]]\n",
    "# data2 = data2_in[[c for c in data2_in.columns if c not in drop_cols and c in select_cols]]\n",
    "# data3 = data3_in[[c for c in data3_in.columns if c not in drop_cols and c in select_cols]]\n",
    "\n",
    "data1 = data1_in[[c for c in data1_in.columns if c not in drop_cols]]\n",
    "data2 = data2_in[[c for c in data2_in.columns if c not in drop_cols]]\n",
    "data3 = data3_in[[c for c in data3_in.columns if c not in drop_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22382, 138)\n",
      "(26468, 138)\n",
      "(51480, 137)\n"
     ]
    }
   ],
   "source": [
    "print (data1.shape)\n",
    "print (data2.shape)\n",
    "print (data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 目前线上的参数，调过\n",
    "# LGBM = lgb.LGBMClassifier(  max_depth=7,\n",
    "#                             n_estimators = 220,\n",
    "#                             learning_rate =0.1,     \n",
    "#                             objective = 'binary',\n",
    "#                             num_leaves=20,\n",
    "#                             boosting_type = 'dart',\n",
    "#                             feature_fraction=0.5,\n",
    "#                             lambda_l1=1,\n",
    "#                             lambda_l2=0.1,\n",
    "#                             subsample=0.7,\n",
    "#                             max_bin=25,\n",
    "#                             min_data_in_leaf=20\n",
    "# )\n",
    "LGBM = lgb.LGBMClassifier(  max_depth=6,\n",
    "                            n_estimators = 280,\n",
    "                            learning_rate =0.05,     \n",
    "                            objective = 'binary',\n",
    "                            num_leaves=25,\n",
    "                            boosting_type = 'dart',\n",
    "                            feature_fraction=0.5,\n",
    "                            lambda_l1=1,\n",
    "                            lambda_l2=0.5,\n",
    "                            subsample=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 题目规定线下 F1计算方法\n",
    "def sroceF1(pred, real):\n",
    "    M = set(pred)\n",
    "    N = set(real)\n",
    "    Precision = len(M.intersection(N))/len(M)\n",
    "    Recall = len(M.intersection(N))/len(N)\n",
    "    F1 = 2*Precision*Recall/(Precision+Recall)\n",
    "\n",
    "    print(\"Precision=\",Precision,\"| Recall=\",Recall)\n",
    "    print(\"F1=\",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练模型，做预测\n",
    "def buildModelAndPredict(isOnLine=True, isTest=False, yuzhi=0.4, model=LGBM):\n",
    "    # 线上预测\n",
    "    # @return: 返回活跃用户\n",
    "    if (isOnLine):\n",
    "        train = pd.concat([data1, data2])\n",
    "        test = data3.copy()\n",
    "        train.pop('user_id')\n",
    "        label = train.pop('label')\n",
    "        \n",
    "        model.fit(train, label)\n",
    "        user_list = test.pop('user_id')\n",
    "        user_df = pd.DataFrame(user_list)\n",
    "        user_df['pre_act'] = model.predict_proba(test)[:,1]\n",
    "        return user_df\n",
    "\n",
    "    # 线下训练调试\n",
    "    else: \n",
    "        train = data1.copy()\n",
    "        test = data2.copy()\n",
    "        train.pop('user_id')\n",
    "        train_df_label = train.pop('label')\n",
    "        train_df = train\n",
    "        \n",
    "        real_user = test[test.label==1]['user_id']\n",
    "        user_list = test.pop('user_id')\n",
    "        test.pop('label')\n",
    "        test_df = test\n",
    "        \n",
    "        user_df = pd.DataFrame(user_list)\n",
    "        model.fit(train_df, train_df_label)\n",
    "        user_df['pre_act'] = model.predict_proba(test_df)[:,1]\n",
    "        \n",
    "        user_pre = user_df[user_df.pre_act>yuzhi]['user_id']\n",
    "        sroceF1(user_pre, real_user)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision= 0.76398335864756 | Recall= 0.8580434621375065\n",
      "F1= 0.8082861734087892\n",
      "0.39\n",
      "Precision= 0.7652081816376515 | Recall= 0.8573759549061781\n",
      "F1= 0.8086743616649178\n",
      "0.392\n",
      "Precision= 0.76576277928794 | Recall= 0.8566342802047022\n",
      "F1= 0.8086536441923966\n",
      "0.394\n",
      "Precision= 0.7665426521392505 | Recall= 0.8557442705629311\n",
      "F1= 0.8086910811284388\n",
      "0.396\n",
      "Precision= 0.7673303589265499 | Recall= 0.8546317585107172\n",
      "F1= 0.8086315789473684\n",
      "0.398\n",
      "Precision= 0.7680656569026489 | Recall= 0.853741748868946\n",
      "F1= 0.808640674394099\n",
      "0.4\n",
      "Precision= 0.7689786636345395 | Recall= 0.8527034042868797\n",
      "F1= 0.8086797495955546\n",
      "0.402\n",
      "Precision= 0.7696124446828483 | Recall= 0.8512942223540755\n",
      "F1= 0.8083952530196853\n",
      "0.404\n",
      "Precision= 0.7700107440236369 | Recall= 0.850478380182452\n",
      "F1= 0.8082466960352422\n",
      "0.406\n",
      "Precision= 0.7710843373493976 | Recall= 0.8496625380108285\n",
      "F1= 0.808468595624559\n",
      "0.408\n",
      "Precision= 0.7716269975052256 | Recall= 0.8487725283690574\n",
      "F1= 0.8083633538178993\n",
      "0.41\n",
      "Precision= 0.7728900601391986 | Recall= 0.8483275235481718\n",
      "F1= 0.8088536878580016\n",
      "0.412\n",
      "Precision= 0.7736628300609343 | Recall= 0.8475116813765482\n",
      "F1= 0.8089052490001061\n",
      "0.414\n",
      "Precision= 0.7746708293742365 | Recall= 0.8465475042646295\n",
      "F1= 0.8090158415139809\n",
      "0.416\n",
      "Precision= 0.7752702059683231 | Recall= 0.8458799970333012\n",
      "F1= 0.8090373838405335\n",
      "0.418\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.39, 0.42, 0.002):\n",
    "    user_df = buildModelAndPredict(isOnLine=False, isTest=False, yuzhi=i, model=LGBM)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df = buildModelAndPredict(isOnLine=True, isTest=False, model=LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 79, 194, 422,  43, 300, 234, 149,  52,  65, 178, 460,  68,  54,\n",
       "        39,  21,  34,  12,  39,  16,  13,  20,   6,  20,  13,   0,  56,\n",
       "       193, 130,  43,  96,  86,  53,  60,  76,  27,  21,  53,  32,  11,\n",
       "        71,  23,  10,   2,  34,  16,  23,  12,  15,   8,  23,   6,   5,\n",
       "         1,  46,  13,  37,  17,  26,  11,  33,  11,  11,   4,  59,  63,\n",
       "        47,  92,  39,  63,  95,  29,  45,   7,  85,   8,   2,   0,   0,\n",
       "         0,  50,  21,  12,   5,   6,   5,   0,   0,   0,   0,   0,   0,\n",
       "        70,  36,  17,   6,   8,   8,   4,   0,   0,   0,   0,   0,  77,\n",
       "        97,  64,  37,   0,   0, 134,  23,   9,  12,   0,   0,   2, 171,\n",
       "       114,  94,  77,  29,  11, 138,  70,  86,  95,  49,  37,  63,  44,\n",
       "        52,  27,  39,  48, 114,  99])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_pre = user_df.sort_values(by='pre_act', axis = 0, ascending = False)['user_id'].head(24500)\n",
    "user_pre = user_df[user_df.pre_act>0.4]['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24379"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df.to_csv(r'..\\Data\\out_data\\user_out.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果数据提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_pre.to_csv('../Output/0627_136_808640_24379_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// 复现82Line\n",
    "116_80236_24870_40 线上：0.820298\n",
    "\n",
    "// 82Line +resiter_day原数据 fillna（0）\n",
    "117_80263_24890_40 线上：0.819631\n",
    "\n",
    "// 82Line -page_sum和action_sum fillna（0）\n",
    "114_8019_24781_40  线上：0.820071\n",
    "\n",
    "// 82Line +day_diff\n",
    "134_80236_24991_40 线上：0.820108\n",
    "\n",
    "// 82Line -page_sum和action_sum,不作fillna\n",
    "114_8024099_24712_404  线上：0.820083         0.4:0.80188; 0.404:0.8024099"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "0624:\n",
    "\n",
    "// +cnt_diff\n",
    "0624_126_80767_24746_4 线上：0.820209                  0.4:0.80767\n",
    "\n",
    "// +day_diff\n",
    "0624_132_802742_24959_4  线上：0.819880              0.4:0.802742; 0.404:0.802946\n",
    "\n",
    "// +retention\n",
    "0624_124_802092_25073_4  线上: 0.819508              0.4:0.802092; 0.416:0.8027104\n",
    "\n",
    "// +a_va_set_cnt_data\n",
    "0624_122_80156_24954_4  线上:  0.820600            0.4:0.801560; 0.412:0.8024499\n",
    "\n",
    "// +all\n",
    "0624_162_8066504_24906_4 线上: 0.820019            0.4:0.80665040; 0.414:0.778059\n",
    "\n",
    "// +all 调参\n",
    "0626_162_807501_24624_4 线上:             0.4:0.807501; 0.394:0.807855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_data1 = pd.read_csv(r'..\\Data\\out_data\\xgb_result_0625.csv')\n",
    "out_data2 = pd.read_csv(r'..\\Data\\out_data\\user_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_user = pd.merge(out_data1, out_data2, on='user_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_user['score'] = out_user['label']*0.4 + out_user['pre_act']*0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = out_user[out_user.score>0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24646"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users.to_csv(r'../Output/rohe_24646_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
