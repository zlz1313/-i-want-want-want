{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv('../Data/raw_data/user_register_log.txt', header=None,names=['user_id','register_day','register_type','device_type'],sep='\\t') # 注册用户数据加载\n",
    "user_login = pd.read_csv('../Data/raw_data/app_launch_log.txt', header=None,names=['user_id','day'],sep='\\t') # app登录日志数据加载\n",
    "user_act = pd.read_csv('../Data/raw_data/user_activity_log.txt', header=None,names=['user_id','day','page','video_id','author_id','action_type'],sep='\\t') # 用户行为日志数据加载\n",
    "user_video = pd.read_csv('../Data/raw_data/video_create_log.txt', header=None,names=['user_id','day'],sep='\\t') # 用户拍摄视频日志数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有数据都是1-30天内的数据\n",
    "\n",
    "且无缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据进行滑窗法划分："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: \n",
    "\n",
    "    +-data1\n",
    "        --feature:1-15;label:16-22\n",
    "    +-data2\n",
    "        --feature:9-23;label:24-30\n",
    "    +-data3\n",
    "        --feature:15-30;label: 31-37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cutDataFunc(data, cut_col ,start_day, end_day):\n",
    "    return data[(data[cut_col]<=end_day)&(data[cut_col]>=start_day)]\n",
    "\n",
    "def cutDataByTime(data_url, start_day, end_day):\n",
    "    temp_users = cutDataFunc(users, 'register_day', start_day, end_day)\n",
    "    temp_login = cutDataFunc(user_login, 'day', start_day, end_day)\n",
    "    temp_act = cutDataFunc(user_act, 'day', start_day, end_day)\n",
    "    temp_video = cutDataFunc(user_video, 'day', start_day, end_day)\n",
    "    \n",
    "    temp_users.to_csv(data_url+'users.csv',index=False)\n",
    "    temp_login.to_csv(data_url+'login.csv',index=False)\n",
    "    temp_act.to_csv(data_url+'act.csv',index=False)\n",
    "    temp_video.to_csv(data_url+'video.csv',index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutDataProgram():\n",
    "    print (\"---------START-----------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data1/train_', 1, 15)\n",
    "    cutDataByTime('../Data/data1/test_', 16, 22)\n",
    "    print (\"-----第1数据集完成-------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data2/train_', 9, 23)\n",
    "    cutDataByTime('../Data/data2/test_', 24, 30)\n",
    "    print (\"-----第2数据集完成-------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data3/train_', 1, 30)\n",
    "    print (\"-----第3数据集完成-------\")\n",
    "    \n",
    "    print (\"----------END------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------START-----------\n",
      "-----第1数据集完成-------\n",
      "-----第2数据集完成-------\n",
      "-----第3数据集完成-------\n",
      "----------END------------\n"
     ]
    }
   ],
   "source": [
    "cutDataProgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train集构造和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCntOfOneDay(data):\n",
    "    return sum(data[(data.day==0)]['cnt'].values)\n",
    "\n",
    "def getCntOfSomeDay(data, day_len=7):\n",
    "    return sum(data[(data.day>=0) & (data.day<day_len)]['cnt'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getActivityUsers(data_url):\n",
    "    test_login = pd.read_csv(data_url+'test_login.csv')\n",
    "    test_act = pd.read_csv(data_url+'test_act.csv')\n",
    "    test_video = pd.read_csv(data_url+'test_video.csv')\n",
    "    \n",
    "    activity_user = np.unique(pd.concat([test_login['user_id'], test_act['user_id'], test_video['user_id']]))\n",
    "    return activity_user\n",
    "\n",
    "# video and lanuch create feature method\n",
    "def getCountFeature(data, name='login'):\n",
    "    day_max = max(data['day'])\n",
    "    data['day'] = day_max - data.day\n",
    "    df = data.groupby(['user_id','day'],as_index=False).apply(lambda x:x.shape[0])\n",
    "    df_temp = pd.DataFrame(df, columns=['cnt']).reset_index()\n",
    "    res_df = df_temp.groupby(['user_id'],as_index=False).agg({'day':['max','min','std'],'cnt':['count','sum','max','var','mean']})\n",
    "    res_df.columns = ['user_id', name+'_day_max',name+'_day_min',name+'_day_std',name+'_cnt',name+'_sum',name+'_max',name+'var',name+'mean']\n",
    "    \n",
    "    res_df[name+'_last_cnt'] = df_temp.groupby(['user_id'], as_index=False).apply(lambda x:getCntOfOneDay(x))\n",
    "    res_df[name+'_3_cnt'] = df_temp.groupby(['user_id'], as_index=False).apply(lambda x:getCntOfSomeDay(x, 3))\n",
    "    res_df[name+'_3_arg_cnt'] = res_df[name+'_3_cnt'] / 3\n",
    "    res_df[name+'_week_cnt'] = df_temp.groupby(['user_id'], as_index=False).apply(lambda x:getCntOfSomeDay(x))\n",
    "    res_df[name+'_week_arg_cnt'] = res_df[name+'_week_cnt'] / 7\n",
    "    return res_df.fillna(0)\n",
    "\n",
    "# act: create feature method\n",
    "def getCountFeatureAboutAct(data, name='act'):\n",
    "    day_max = max(data['day'])\n",
    "    data['day'] = day_max - data.day\n",
    "    authors = set(data['author_id'])\n",
    "    \n",
    "    df = data.groupby(['user_id','day'],as_index=False).apply(lambda x:x.shape[0])\n",
    "    df_temp = pd.DataFrame(df, columns=['cnt']).reset_index()\n",
    "    res_df = df_temp.groupby(['user_id'],as_index=False).agg({'day':['max','min','std'],'cnt':['count','sum','max','var','mean']})\n",
    "    res_df.columns = ['user_id', name+'_day_max',name+'_day_min',name+'_day_std',name+'_cnt',name+'_sum',name+'_max',name+'var',name+'mean']\n",
    "    \n",
    "    res_df[name+'_last_cnt'] = df_temp.groupby(['user_id'], as_index=False).apply(lambda x:getCntOfOneDay(x))\n",
    "    res_df[name+'_3_cnt'] = df_temp.groupby(['user_id'], as_index=False).apply(lambda x:getCntOfSomeDay(x, 3))\n",
    "    res_df[name+'_3_arg_cnt'] = res_df[name+'_3_cnt'] / 3\n",
    "    res_df[name+'_week_cnt'] = df_temp.groupby(['user_id'], as_index=False).apply(lambda x:getCntOfSomeDay(x))\n",
    "    res_df[name+'_week_arg_cnt'] = res_df[name+'_week_cnt'] / 7\n",
    "    \n",
    "    page = data.groupby(['user_id','page'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "#     page_sum = page[0]+page[1]+page[2]+page[3]+page[4]\n",
    "#     res_df['page_0_sigle'] = page[0] / page_sum\n",
    "#     res_df['page_1_sigle'] = page[1] / page_sum\n",
    "#     res_df['page_2_sigle'] = page[2] / page_sum\n",
    "#     res_df['page_3_sigle'] = page[3] / page_sum\n",
    "#     res_df['page_4_sigle'] = page[4] / page_sum\n",
    "#     res_df['page_0_all'] = page[0] / sum(page[0])\n",
    "#     res_df['page_1_all'] = page[1] / sum(page[1])\n",
    "#     res_df['page_2_all'] = page[2] / sum(page[2])\n",
    "#     res_df['page_3_all'] = page[3] / sum(page[3])\n",
    "#     res_df['page_4_all'] = page[4] / sum(page[4])\n",
    "    res_df[['page_0','page_1','page_2','page_3','page_4']] = page[[0,1,2,3,4]]\n",
    "    action_type = data.groupby(['user_id','action_type'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index()\n",
    "#     action_type_sum = action_type[0]+action_type[1]+action_type[2]+action_type[3]+action_type[4]\n",
    "#     res_df['action_type_0_sigle'] = action_type[0] / action_type_sum\n",
    "#     res_df['action_type_1_sigle'] = action_type[1] / action_type_sum\n",
    "#     res_df['action_type_2_sigle'] = action_type[2] / action_type_sum\n",
    "#     res_df['action_type_3_sigle'] = action_type[3] / action_type_sum\n",
    "#     res_df['action_type_4_sigle'] = action_type[4] / action_type_sum\n",
    "#     res_df['action_type_5_sigle'] = action_type[5] / action_type_sum\n",
    "#     res_df['action_type_0_all'] = action_type[0] / sum(action_type[0])\n",
    "#     res_df['action_type_1_all'] = action_type[1] / sum(action_type[1])\n",
    "#     res_df['action_type_2_all'] = action_type[2] / sum(action_type[2])\n",
    "#     res_df['action_type_3_all'] = action_type[3] / sum(action_type[3])\n",
    "#     res_df['action_type_4_all'] = action_type[4] / sum(action_type[4])\n",
    "#     res_df['action_type_5_all'] = action_type[5] / sum(action_type[5])\n",
    "    res_df[['action_type_0','action_type_1','action_type_2','action_type_3','action_type_4','action_type_5']] = action_type[[0,1,2,3,4,5]]\n",
    "    \n",
    "    res_df['is_author'] = res_df['user_id'].apply(lambda x: 1 if x in authors else 0)\n",
    "    return res_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------Test------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_act = pd.read_csv('../Data/data1/'+'train_act.csv')\n",
    "# res = getCountFeatureAboutAct(train_act)\n",
    "# train_act.set_index('user_id').apply(lambda x:x.diff())\n",
    "page = train_act.groupby(['user_id','page'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = train_act.groupby(['user_id','page'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCntOfSomeDayAct(data, d, day_len=7, col=0):\n",
    "    return pd.DataFrame(data[data.day<=d & data.day>d-day_len].groupby(['user_id']).apply(lambda x:sum(x[col].values)))\n",
    "\n",
    "pd.DataFrame(temp[temp.day==13].groupby(['user_id']).apply(lambda x:sum(x[0].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCntOfSomeDay(data, d, day_len=7):\n",
    "    return sum(data[(data.day<=d) & (data.day>d-day_len)]['0'].values)\n",
    "\n",
    "temp.groupby(['user_id'], as_index=False).apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf = train_act.groupby(['user_id','day'],as_index=False).apply(lambda x:x.shape[0])\n",
    "tdf = pd.DataFrame(tdf, columns=['cnt']).reset_index()\n",
    "res_df = tdf.groupby(['user_id'],as_index=False).agg({'day':['max','min','std','var','mean'],'cnt':['count','sum','max','var','mean']})\n",
    "res_df.columns = ['user_id', 'day_max','day_min','day_std','day_var','day_mean','cnt','sum','max','var','mean']\n",
    "\n",
    "# authors = set(train_act['author_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = train_act.groupby(['user_id','page'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_type = train_act.groupby(['user_id','action_type'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df[['page_0','page_1','page_2','page_3','page_4']] = page[[0,1,2,3,4]]\n",
    "res_df[['action_type_0','action_type_1','action_type_2','action_type_3','action_type_4','action_type_5']] = action_type[[0,1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df['is_author'] = res_df['user_id'].apply(lambda x: 1 if x in authors else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ------------------------------------------------------------- -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constructDataFeature(data_url):\n",
    "    train_login = pd.read_csv(data_url+'train_login.csv')\n",
    "    train_act = pd.read_csv(data_url+'train_act.csv')\n",
    "    train_video = pd.read_csv(data_url+'train_video.csv')\n",
    "    \n",
    "    # register data\n",
    "    train_user = pd.read_csv(data_url+'train_users.csv')\n",
    "    feature = train_user\n",
    "    max_day = max(feature['register_day'])\n",
    "    feature['register_day'] = max_day - feature.register_day\n",
    "    \n",
    "    # login data\n",
    "    # 登录次数、启动天数、平均次数、最大值、最小值\n",
    "    # 连续几天启动总次数、平均次数、前一天次数\n",
    "    train_login_feas = getCountFeature(train_login, 'login')\n",
    "    feature = pd.merge(feature, train_login_feas, on='user_id', how='left')\n",
    "    \n",
    "    # video data\n",
    "    # 登录次数、启动天数、平均次数、最大值、最小值\n",
    "    # 连续几天启动总次数、平均次数、前一天次数\n",
    "    train_video_feas = getCountFeature(train_video, 'video')\n",
    "    feature = pd.merge(feature, train_video_feas, on='user_id', how='left')\n",
    "    \n",
    "    # act data\n",
    "    # 登录次数、启动天数、平均次数、最大值、最小值\n",
    "    # 连续几天启动总次数、平均次数、前一天次数\n",
    "    # page/action_type分组统计特征提取\n",
    "    # author_id分组提取信息（user_id=author_id）\n",
    "    train_act_feas = getCountFeatureAboutAct(train_act, 'act')\n",
    "    feature = pd.merge(feature, train_act_feas, on='user_id', how='left')\n",
    "    \n",
    "    return feature.fillna(0)\n",
    "\n",
    "def getTrainLabel(data_url, data):\n",
    "    # get activity label of train from test_dataset\n",
    "    train_label = []\n",
    "    activity_users = getActivityUsers(data_url)\n",
    "    for u in data['user_id']:\n",
    "        if u in activity_users:\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "    data['label'] = train_label\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataProgram():\n",
    "    print(\"----------------构造训练集-------------------\")\n",
    "    \n",
    "    data_url = '../Data/data1/'\n",
    "    data1 = constructDataFeature(data_url)\n",
    "    data1 = getTrainLabel(data_url, data1)\n",
    "    print(\"---------第一组数据集处理完成----------------\")\n",
    "    data1.to_csv(data_url+'data1.csv',index=False)\n",
    "    \n",
    "    data_url = '../Data/data2/'\n",
    "    data2 = constructDataFeature(data_url)\n",
    "    data2 = getTrainLabel(data_url, data2)\n",
    "    print(\"---------第二组数据集处理完成----------------\")\n",
    "    data2.to_csv(data_url+'data2.csv',index=False)\n",
    "    \n",
    "    data_url = '../Data/data3/'\n",
    "    data3 = constructDataFeature(data_url)\n",
    "    print(\"---------第三组数据集处理完成----------------\")\n",
    "    data3.to_csv(data_url+'data3.csv',index=False)\n",
    "    \n",
    "    print(\"--------------------END----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------构造训练集-------------------\n",
      "---------第一组数据集处理完成----------------\n",
      "---------第二组数据集处理完成----------------\n",
      "---------第三组数据集处理完成----------------\n",
      "--------------------END----------------------\n"
     ]
    }
   ],
   "source": [
    "getDataProgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型并进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_in = pd.read_csv('../Data/data1/data1.csv')\n",
    "data2_in = pd.read_csv('../Data/data2/data2.csv')\n",
    "data3_in = pd.read_csv('../Data/data3/data3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'register_day', 'register_type', 'device_type',\n",
       "       'login_day_max', 'login_day_min', 'login_day_std', 'login_cnt',\n",
       "       'login_sum', 'login_max', 'loginvar', 'loginmean', 'login_last_cnt',\n",
       "       'login_3_cnt', 'login_3_arg_cnt', 'login_week_cnt',\n",
       "       'login_week_arg_cnt', 'video_day_max', 'video_day_min', 'video_day_std',\n",
       "       'video_cnt', 'video_sum', 'video_max', 'videovar', 'videomean',\n",
       "       'video_last_cnt', 'video_3_cnt', 'video_3_arg_cnt', 'video_week_cnt',\n",
       "       'video_week_arg_cnt', 'act_day_max', 'act_day_min', 'act_day_std',\n",
       "       'act_cnt', 'act_sum', 'act_max', 'actvar', 'actmean', 'act_last_cnt',\n",
       "       'act_3_cnt', 'act_3_arg_cnt', 'act_week_cnt', 'act_week_arg_cnt',\n",
       "       'page_0', 'page_1', 'page_2', 'page_3', 'page_4', 'action_type_0',\n",
       "       'action_type_1', 'action_type_2', 'action_type_3', 'action_type_4',\n",
       "       'action_type_5', 'is_author', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_in.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_cols = ['login_sum','login_max','loginvar','loginmean','login_3_cnt','login_week_cnt','device_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select_cols = ['user_id','label'] + ['login_day_min', 'act_last_cnt', 'device_type', 'login_day_std',\n",
    "#        'login_cnt', 'login_3_arg_cnt', 'act_sum', 'login_week_arg_cnt',\n",
    "#        'register_type', 'act_week_cnt', 'act_3_cnt', 'page_1_sigle',\n",
    "#        'page_0_sigle', 'act_max', 'page_1_all', 'page_0_all', 'actvar',\n",
    "#        'act_day_std', 'page_3_sigle', 'actmean', 'act_3_arg_cnt', 'act_cnt',\n",
    "#        'act_week_arg_cnt', 'page_2_sigle', 'act_day_min', 'login_last_cnt',\n",
    "#        'page_4_all', 'page_2_all', 'video_week_cnt', 'videomean',\n",
    "#        'register_day', 'page_3_all', 'login_day_max', 'page_4_sigle',\n",
    "#        'act_day_max', 'video_week_arg_cnt', 'video_3_arg_cnt', 'video_3_cnt',\n",
    "#        'video_last_cnt', 'video_sum', 'video_day_max', 'video_day_std',\n",
    "#        'video_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapDeviceType(thread_value=0.5):\n",
    "    con_data = pd.concat([data1_in, data2_in])\n",
    "    index = con_data['label'].groupby(con_data[\"device_type\"]).mean().index\n",
    "    values = con_data['label'].groupby(con_data[\"device_type\"]).mean().get_values()\n",
    "    return index[values>thread_value]\n",
    "\n",
    "good_index = mapDeviceType()\n",
    "\n",
    "data1_in['device_map'] = data1_in['device_type'].apply(lambda x: int(x in good_index))\n",
    "data2_in['device_map'] = data1_in['device_type'].apply(lambda x: int(x in good_index))\n",
    "data3_in['device_map'] = data1_in['device_type'].apply(lambda x: int(x in good_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data1 = data1_in[[c for c in data1_in.columns if c not in drop_cols and c in select_cols]]\n",
    "# data2 = data2_in[[c for c in data2_in.columns if c not in drop_cols and c in select_cols]]\n",
    "# data3 = data3_in[[c for c in data3_in.columns if c not in drop_cols and c in select_cols]]\n",
    "\n",
    "data1 = data1_in[[c for c in data1_in.columns if c not in drop_cols]]\n",
    "data2 = data2_in[[c for c in data2_in.columns if c not in drop_cols]]\n",
    "data3 = data3_in[[c for c in data3_in.columns if c not in drop_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22342, 50)\n",
      "(26571, 50)\n",
      "(51709, 49)\n"
     ]
    }
   ],
   "source": [
    "print (data1.shape)\n",
    "print (data2.shape)\n",
    "print (data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LGBM = lgb.LGBMClassifier(  max_depth=6,\n",
    "            n_estimators = 280,\n",
    "            learning_rate =0.05,     \n",
    "            objective = 'binary',\n",
    "            num_leaves=25,\n",
    "            boosting_type = 'dart',\n",
    "            feature_fraction=0.5,\n",
    "            lambda_l1=1,\n",
    "            lambda_l2=0.5,\n",
    "            subsample=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sroceF1(pred, real):\n",
    "    M = set(pred)\n",
    "    N = set(real)\n",
    "    Precision = len(M.intersection(N))/len(M)\n",
    "    Recall = len(M.intersection(N))/len(N)\n",
    "    F1 = 2*Precision*Recall/(Precision+Recall)\n",
    "\n",
    "    print(\"Precision=\",Precision,\"| Recall=\",Recall)\n",
    "    print(\"F1=\",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildModelAndPredict(isOnLine=True, isTest=False, yuzhi=0.5, model=lgb.LGBMClassifier(  max_depth=3,\n",
    "                                                                    n_estimators = 120,\n",
    "                                                                    learning_rate = 0.05,     \n",
    "                                                                    objective = 'binary',\n",
    "                                                                    subsample = 0.7,\n",
    "                                                                    colsample_bytree = 0.74,\n",
    "                                                                    num_leaves = 8)\n",
    "                        ):\n",
    "    if (isOnLine):\n",
    "        # yuzhi=0.4\n",
    "        train = pd.concat([data1, data2])\n",
    "        test = data3.copy()\n",
    "        train.pop('user_id')\n",
    "        label = train.pop('label')\n",
    "        \n",
    "        model.fit(train, label)\n",
    "        user_list = test.pop('user_id')\n",
    "        print (len(user_list))\n",
    "        user_df = pd.DataFrame(user_list)\n",
    "        user_df['pre_act'] = model.predict_proba(test)[:,1]\n",
    "        return user_df[user_df.pre_act>yuzhi]['user_id']\n",
    "            \n",
    "    else: \n",
    "        # best yuzhi 0.6\n",
    "        train = data1.copy()\n",
    "        test = data2.copy()\n",
    "        # train pop user_id and get label\n",
    "        train.pop('user_id')\n",
    "        train_df_label = train.pop('label')\n",
    "        train_df = train\n",
    "        \n",
    "        # test get user_id and pop label\n",
    "        real_user = test[test.label==1]['user_id']\n",
    "        user_list = test.pop('user_id')\n",
    "        test.pop('label')\n",
    "        test_df = test\n",
    "        \n",
    "        user_df = pd.DataFrame(user_list)\n",
    "        # train the model and predict\n",
    "        model.fit(train_df, train_df_label)\n",
    "        user_df['pre_act'] = model.predict_proba(test_df)[:,1]\n",
    "        \n",
    "        # calculate the F1 score\n",
    "        if (isTest):\n",
    "            for i in np.arange(0.3, 0.8, 0.01):\n",
    "                user_pre = user_df[user_df.pre_act>i]['user_id']\n",
    "                sroceF1(user_pre, real_user)\n",
    "                print (i)\n",
    "        else:\n",
    "            user_pre = user_df[user_df.pre_act>yuzhi]['user_id']\n",
    "            print (len(user_pre),len(real_user))\n",
    "            sroceF1(user_pre, real_user)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51709\n"
     ]
    }
   ],
   "source": [
    "user_pre = buildModelAndPredict(isOnLine=True, isTest=False, yuzhi=0.40, model=LGBM)   #0.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24412"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果数据提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_pre.to_csv('../Output/result_down0001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
