{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv('../Data/raw_data/user_register_log.txt', header=None,names=['user_id','register_day','register_type','device_type'],sep='\\t') # 注册用户数据加载\n",
    "user_login = pd.read_csv('../Data/raw_data/app_launch_log.txt', header=None,names=['user_id','day'],sep='\\t') # app登录日志数据加载\n",
    "user_act = pd.read_csv('../Data/raw_data/user_activity_log.txt', header=None,names=['user_id','day','page','video_id','author_id','action_type'],sep='\\t') # 用户行为日志数据加载\n",
    "user_video = pd.read_csv('../Data/raw_data/video_create_log.txt', header=None,names=['user_id','day'],sep='\\t') # 用户拍摄视频日志数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有数据都是1-30天内的数据\n",
    "\n",
    "且无缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据进行滑窗法划分："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: \n",
    "\n",
    "    +-data1 1-15\n",
    "        --feature:1-15;label:16-22\n",
    "    +-data2 1-23\n",
    "        --feature:9-23;label:24-30\n",
    "    +-data3 1-30\n",
    "        --feature:16-30;label: 31-37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cutDataFunc(data, cut_col ,start_day, end_day):\n",
    "    return data[(data[cut_col]<=end_day)&(data[cut_col]>=start_day)]\n",
    "\n",
    "def cutDataByTime(data_url, start_day, end_day):\n",
    "    temp_users = cutDataFunc(users, 'register_day', 1, end_day)\n",
    "    temp_login = cutDataFunc(user_login, 'day', start_day, end_day)\n",
    "    temp_act = cutDataFunc(user_act, 'day', start_day, end_day)\n",
    "    temp_video = cutDataFunc(user_video, 'day', start_day, end_day)\n",
    "    \n",
    "    temp_users.to_csv(data_url+'users.csv',index=False)\n",
    "    temp_login.to_csv(data_url+'login.csv',index=False)\n",
    "    temp_act.to_csv(data_url+'act.csv',index=False)\n",
    "    temp_video.to_csv(data_url+'video.csv',index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutDataProgram():\n",
    "    start = time.time()\n",
    "    print (\"---------START-----------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data1/train_', 1, 15)\n",
    "    cutDataByTime('../Data/data1/test_', 16, 22)\n",
    "    print (\"-----第1数据集完成-------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data2/train_', 9, 23)\n",
    "    cutDataByTime('../Data/data2/test_', 24, 30)\n",
    "    print (\"-----第2数据集完成-------\")\n",
    "    \n",
    "    cutDataByTime('../Data/data3/train_', 16, 30)\n",
    "    print (\"-----第3数据集完成-------\")\n",
    "    \n",
    "    print (\"----------END------------\")\n",
    "    end = time.time()\n",
    "    print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------START-----------\n",
      "-----第1数据集完成-------\n",
      "-----第2数据集完成-------\n",
      "-----第3数据集完成-------\n",
      "----------END------------\n",
      "199.9984393119812\n"
     ]
    }
   ],
   "source": [
    "cutDataProgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train集构造和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取有活跃行为的用户集，（从test集中获取）\n",
    "def getActivityUsers(data_url):\n",
    "    test_login = pd.read_csv(data_url+'test_login.csv')\n",
    "    test_act = pd.read_csv(data_url+'test_act.csv')\n",
    "    test_video = pd.read_csv(data_url+'test_video.csv')\n",
    "    \n",
    "    activity_user = np.unique(pd.concat([test_login['user_id'], test_act['user_id'], test_video['user_id']]))\n",
    "    return activity_user\n",
    "\n",
    "def get_diff_from_ls(x):\n",
    "    x.sort()\n",
    "    return list(np.diff(x))\n",
    "\n",
    "# （video and lanuch）data create feature method\n",
    "# 1、将day转换成距离时间窗口截点的距离\n",
    "# 2、描述性day、cnt统计特征\n",
    "# 3、连续1、 2、 3、 7天内的统计特征\n",
    "def getCountFeature(data, name='login'):\n",
    "    day_max = max(data['day'])\n",
    "    data['day'] = day_max - data.day\n",
    "    df = data.groupby(['user_id','day'],as_index=False).apply(lambda x:x.shape[0])\n",
    "    df_temp = pd.DataFrame(df, columns=['cnt']).reset_index()\n",
    "    \n",
    "    df_temp_group = df_temp.groupby(['user_id'],as_index=False)\n",
    "    res_df = df_temp_group.agg({'day':['max','min','std'],'cnt':['count','sum','max','var','mean']})\n",
    "    res_df.columns = ['user_id', name+'_day_max',name+'_day_min',name+'_day_std',name+'_cnt',name+'_sum',name+'_max',name+'_var',name+'_mean']\n",
    "    \n",
    "    # 日期差分统计特征\n",
    "#     df_temp_diff = df_temp[['user_id','day']].groupby('user_id').aggregate(lambda x: list(set(x)))\n",
    "#     df_temp_diff.day = df_temp_diff.day.apply(lambda x:get_diff_from_ls(x))\n",
    "#     res_df[name+'_day_diff_max'] = df_temp_diff.day.apply(lambda x: max(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_min'] = df_temp_diff.day.apply(lambda x: min(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_mean'] = df_temp_diff.day.apply(lambda x: np.mean(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_std'] = df_temp_diff.day.apply(lambda x: pd.Series(x).std() if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_skew'] = df_temp_diff.day.apply(lambda x: pd.Series(x).skew()).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_kurt'] = df_temp_diff.day.apply(lambda x: pd.Series(x).kurt()).fillna(-1).values\n",
    "    \n",
    "    \n",
    "    for day_len in [1,3,7]:\n",
    "        df_temp_day = df_temp[(df_temp.day>=0) & (df_temp.day<day_len)]\n",
    "        df_temp_day_cnt = df_temp_day.groupby(['user_id']).apply(lambda x:sum(x['cnt'].values))\n",
    "        add_df = pd.DataFrame(df_temp_day_cnt, columns=[name+'_'+str(day_len)+'_cnt']).reset_index()\n",
    "        res_df = pd.merge(res_df, add_df, on='user_id', how='left').fillna(0)\n",
    "        if day_len != 1:\n",
    "            res_df[name+'_'+str(day_len)+'_cnt_arg'] = res_df[name+'_'+str(day_len)+'_cnt'] / day_len\n",
    "    return res_df.fillna(0)\n",
    "\n",
    "# act: create feature method\n",
    "# 1、将day转换成距离时间窗口截点的距离\n",
    "# 2、描述性day、cnt统计特征\n",
    "# 3、连续1、 2、 3、 7天内的统计特征\n",
    "# 4、page和action_type不同类型的所有统计数以及比率特征\n",
    "# 5、page和action_type不同类型在连续1、 3、 7天内的统计特征\n",
    "# 6、user_id是否为author_id的成员\n",
    "def getCountFeatureAboutAct(data, name='act'):\n",
    "    day_max = max(data['day'])\n",
    "    data['day'] = day_max - data.day\n",
    "    authors = set(data['author_id'])\n",
    "    \n",
    "    df = data.groupby(['user_id','day'],as_index=False).apply(lambda x:x.shape[0])\n",
    "    df_temp = pd.DataFrame(df, columns=['cnt']).reset_index()\n",
    "    \n",
    "    df_temp_group = df_temp.groupby(['user_id'],as_index=False)\n",
    "    res_df = df_temp_group.agg({'day':['max','min','std'],'cnt':['count','sum','max','var','mean']})\n",
    "    res_df.columns = ['user_id', name+'_day_max',name+'_day_min',name+'_day_std',name+'_cnt',name+'_sum',name+'_max',name+'_var',name+'_mean']\n",
    "    \n",
    "    # 日期差分统计特征\n",
    "#     df_temp_diff = df_temp[['user_id','day']].groupby('user_id').aggregate(lambda x: list(set(x)))\n",
    "#     df_temp_diff.day = df_temp_diff.day.apply(lambda x:get_diff_from_ls(x))\n",
    "#     res_df[name+'_day_diff_max'] = df_temp_diff.day.apply(lambda x: max(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_min'] = df_temp_diff.day.apply(lambda x: min(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_mean'] = df_temp_diff.day.apply(lambda x: np.mean(x) if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_std'] = df_temp_diff.day.apply(lambda x: pd.Series(x).std() if len(x)!=0 else -1).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_skew'] = df_temp_diff.day.apply(lambda x: pd.Series(x).skew()).fillna(-1).values\n",
    "#     res_df[name+'_day_diff_kurt'] = df_temp_diff.day.apply(lambda x: pd.Series(x).kurt()).fillna(-1).values\n",
    "    \n",
    "    for day_len in [1,3,7]:\n",
    "        df_temp_day = df_temp[(df_temp.day>=0) & (df_temp.day<day_len)]\n",
    "        df_temp_day_cnt = df_temp_day.groupby(['user_id']).apply(lambda x:sum(x['cnt'].values))\n",
    "        add_df = pd.DataFrame(df_temp_day_cnt, columns=[name+'_'+str(day_len)+'_cnt']).reset_index()\n",
    "        res_df = pd.merge(res_df, add_df, on='user_id', how='left').fillna(0)\n",
    "        if day_len != 1:\n",
    "            res_df[name+'_'+str(day_len)+'_cnt_arg'] = res_df[name+'_'+str(day_len)+'_cnt'] / day_len\n",
    "    \n",
    "    # page 处理\n",
    "    temp = data.groupby(['user_id','day','page'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "    for day_len in [1,3,7]:\n",
    "        for col in [0,1,2,3,4]:\n",
    "            temp_day = temp[(temp.day>=0) & (temp.day<day_len)]\n",
    "            df_temp_day_cnt = temp_day.groupby(['user_id']).apply(lambda x:sum(x[col].values))\n",
    "            add_df = pd.DataFrame(df_temp_day_cnt, columns=['page_'+str(col)+'_'+str(day_len)+'_cnt']).reset_index()\n",
    "            res_df = pd.merge(res_df, add_df, on='user_id', how='left').fillna(0)\n",
    "            if day_len != 1:\n",
    "                res_df['page_'+str(col)+'_'+str(day_len)+'_cnt_arg'] = res_df['page_'+str(col)+'_'+str(day_len)+'_cnt'] / day_len\n",
    "    \n",
    "    page = data.groupby(['user_id','page'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "    page_sum = page[0]+page[1]+page[2]+page[3]+page[4]\n",
    "    res_df['page_0_sigle'] = page[0] / page_sum\n",
    "    res_df['page_1_sigle'] = page[1] / page_sum\n",
    "    res_df['page_2_sigle'] = page[2] / page_sum\n",
    "    res_df['page_3_sigle'] = page[3] / page_sum\n",
    "    res_df['page_4_sigle'] = page[4] / page_sum\n",
    "#     res_df['page_sum'] = page_sum\n",
    "    res_df[['page_0','page_1','page_2','page_3','page_4']] = page[[0,1,2,3,4]]\n",
    "    \n",
    "    # action_type处理\n",
    "    temp = data.groupby(['user_id','day','action_type'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "    temp_group_action = temp.groupby(['user_id'])\n",
    "    for day_len in [1,3,7]:\n",
    "        for col in [0,1,2,3,4,5]:\n",
    "            temp_day = temp[(temp.day>=0) & (temp.day<day_len)]\n",
    "            df_temp_day_cnt = temp_day.groupby(['user_id']).apply(lambda x:sum(x[col].values))\n",
    "            add_df = pd.DataFrame(df_temp_day_cnt, columns=['action_type_'+str(col)+'_'+str(day_len)+'_cnt']).reset_index()\n",
    "            res_df = pd.merge(res_df, add_df, on='user_id', how='left').fillna(0)\n",
    "            if day_len != 1:\n",
    "                res_df['action_type_'+str(col)+'_'+str(day_len)+'_cnt_arg'] = res_df['action_type_'+str(col)+'_'+str(day_len)+'_cnt'] / day_len\n",
    "    \n",
    "    action_type = data.groupby(['user_id','action_type'],as_index=False).apply(lambda x:x.shape[0]).unstack().reset_index().fillna(0)\n",
    "    action_type_sum = action_type[0]+action_type[1]+action_type[2]+action_type[3]+action_type[4]+action_type[5]\n",
    "    res_df['action_type_0_sigle'] = action_type[0] / action_type_sum\n",
    "    res_df['action_type_1_sigle'] = action_type[1] / action_type_sum\n",
    "    res_df['action_type_2_sigle'] = action_type[2] / action_type_sum\n",
    "    res_df['action_type_3_sigle'] = action_type[3] / action_type_sum\n",
    "    res_df['action_type_4_sigle'] = action_type[4] / action_type_sum\n",
    "    res_df['action_type_5_sigle'] = action_type[5] / action_type_sum\n",
    "    #     res_df['action_type_sum'] = action_type_sum\n",
    "    res_df[['action_type_0','action_type_1','action_type_2','action_type_3','action_type_4','action_type_5']] = action_type[[0,1,2,3,4,5]]\n",
    "    \n",
    "    res_df['is_author'] = res_df['user_id'].apply(lambda x: 1 if x in authors else 0)\n",
    "    \n",
    "    return res_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取最近一天的点击数量\n",
    "def getCntOfOneDay(data):\n",
    "    return sum(data[(data.day==0)]['cnt'].values)\n",
    "\n",
    "# 获取最近一段时间内的点击数量\n",
    "def getCntOfSomeDay(data, day_len=7):\n",
    "    return sum(data[(data.day>=0) & (data.day<day_len)]['cnt'].values)\n",
    "\n",
    "# 获取最近一段时间内的点击数量（改进版，可以自由选择col统计）\n",
    "def getCntOfSomeDayWithCol(data, day_len=1, col='page'):\n",
    "    return sum(data[(data.day>=0) & (data.day<day_len)][col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过video 、 act 、 register 、 lauch来构造特征\n",
    "def constructDataFeature(data_url):\n",
    "    train_login = pd.read_csv(data_url+'train_login.csv')\n",
    "    train_act = pd.read_csv(data_url+'train_act.csv')\n",
    "    train_video = pd.read_csv(data_url+'train_video.csv')\n",
    "    \n",
    "    # register data\n",
    "    train_user = pd.read_csv(data_url+'train_users.csv')\n",
    "    feature = train_user\n",
    "    max_day = max(feature['register_day'])\n",
    "    feature['register_day'] = max_day - feature.register_day  # day改成时间窗口截点距离\n",
    "#     feature['register_day_diff'] = max_day - feature.register_day  # day改成时间窗口截点距离\n",
    "    \n",
    "    # login data\n",
    "    train_login_feas = getCountFeature(train_login, 'login')\n",
    "    feature = pd.merge(feature, train_login_feas, on='user_id', how='left')\n",
    "    \n",
    "    # video data\n",
    "    train_video_feas = getCountFeature(train_video, 'video')\n",
    "    feature = pd.merge(feature, train_video_feas, on='user_id', how='left')\n",
    "    \n",
    "    # act data\n",
    "    train_act_feas = getCountFeatureAboutAct(train_act, 'act')\n",
    "    feature = pd.merge(feature, train_act_feas, on='user_id', how='left')\n",
    "    \n",
    "#     feature[ [c for c in feature.columns if 'day_diff' in c ] ] = feature[ [c for c in feature.columns if 'day_diff' in c ] ].fillna(-1)\n",
    "    \n",
    "    # 缺失值全部补NAN\n",
    "    return feature\n",
    "\n",
    "# 通过TEST未来几天内的活跃用户来给Train集标签\n",
    "def getTrainLabel(data_url, data):\n",
    "    # get activity label of train from test_dataset\n",
    "    train_label = []\n",
    "    activity_users = getActivityUsers(data_url)\n",
    "    for u in data['user_id']:\n",
    "        if u in activity_users:\n",
    "            train_label.append(1)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "    data['label'] = train_label\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataProgram():\n",
    "    start = time.time()\n",
    "    print(\"----------------构造训练集-------------------\")\n",
    "    \n",
    "    data_url = '../Data/data1/'\n",
    "    data1 = constructDataFeature(data_url)\n",
    "    data1 = getTrainLabel(data_url, data1)\n",
    "    print(\"---------第一组数据集处理完成----------------\")\n",
    "    data1.to_csv(data_url+'data1.csv',index=False)\n",
    "    end = time.time()\n",
    "    print (end - start)\n",
    "    \n",
    "    data_url = '../Data/data2/'\n",
    "    data2 = constructDataFeature(data_url)\n",
    "    data2 = getTrainLabel(data_url, data2)\n",
    "    print(\"---------第二组数据集处理完成----------------\")\n",
    "    data2.to_csv(data_url+'data2.csv',index=False)\n",
    "    end = time.time()\n",
    "    print (end - start)\n",
    "    \n",
    "    data_url = '../Data/data3/'\n",
    "    data3 = constructDataFeature(data_url)\n",
    "    print(\"---------第三组数据集处理完成----------------\")\n",
    "    data3.to_csv(data_url+'data3.csv',index=False)\n",
    "    \n",
    "    print(\"--------------------END----------------------\")\n",
    "    end = time.time()\n",
    "    print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------构造训练集-------------------\n",
      "---------第一组数据集处理完成----------------\n",
      "39.82827806472778\n",
      "---------第二组数据集处理完成----------------\n",
      "110.1863021850586\n",
      "---------第三组数据集处理完成----------------\n",
      "--------------------END----------------------\n",
      "197.82431507110596\n"
     ]
    }
   ],
   "source": [
    "getDataProgram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型并进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_in = pd.read_csv('../Data/data1/data1.csv')\n",
    "data2_in = pd.read_csv('../Data/data2/data2.csv')\n",
    "data3_in = pd.read_csv('../Data/data3/data3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37446, 122)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../Data/cnt_diff_data/'\n",
    "feature_data1 = pd.read_csv(path + 'data1.csv')\n",
    "feature_data2 = pd.read_csv(path + 'data2.csv')\n",
    "feature_data3 = pd.read_csv(path + 'data3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_in = pd.merge(data1_in, feature_data1, on='user_id', how='left')\n",
    "data2_in = pd.merge(data2_in, feature_data2, on='user_id', how='left')\n",
    "data3_in = pd.merge(data3_in, feature_data3, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cates = pd.Categorical(data3_in['device_type'])\n",
    "categories = cates.categories\n",
    "data3_in['device_type'] = cates.codes\n",
    "data1_in['device_type'] = data1_in['device_type'].apply(lambda x:categories.get_loc(x))\n",
    "data2_in['device_type'] = data2_in['device_type'].apply(lambda x:categories.get_loc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_id', 'register_day', 'register_type', 'device_type', 'login_day_max']\n",
      "['login_day_min', 'login_day_std', 'login_cnt', 'login_sum', 'login_max']\n",
      "['login_var', 'login_mean', 'login_1_cnt', 'login_3_cnt', 'login_3_cnt_arg']\n",
      "['login_7_cnt', 'login_7_cnt_arg', 'video_day_max', 'video_day_min', 'video_day_std']\n",
      "['video_cnt', 'video_sum', 'video_max', 'video_var', 'video_mean']\n",
      "['video_1_cnt', 'video_3_cnt', 'video_3_cnt_arg', 'video_7_cnt', 'video_7_cnt_arg']\n",
      "['act_day_max', 'act_day_min', 'act_day_std', 'act_cnt', 'act_sum']\n",
      "['act_max', 'act_var', 'act_mean', 'act_1_cnt', 'act_3_cnt']\n",
      "['act_3_cnt_arg', 'act_7_cnt', 'act_7_cnt_arg', 'page_0_1_cnt', 'page_1_1_cnt']\n",
      "['page_2_1_cnt', 'page_3_1_cnt', 'page_4_1_cnt', 'page_0_3_cnt', 'page_0_3_cnt_arg']\n",
      "['page_1_3_cnt', 'page_1_3_cnt_arg', 'page_2_3_cnt', 'page_2_3_cnt_arg', 'page_3_3_cnt']\n",
      "['page_3_3_cnt_arg', 'page_4_3_cnt', 'page_4_3_cnt_arg', 'page_0_7_cnt', 'page_0_7_cnt_arg']\n",
      "['page_1_7_cnt', 'page_1_7_cnt_arg', 'page_2_7_cnt', 'page_2_7_cnt_arg', 'page_3_7_cnt']\n",
      "['page_3_7_cnt_arg', 'page_4_7_cnt', 'page_4_7_cnt_arg', 'page_0_sigle', 'page_1_sigle']\n",
      "['page_2_sigle', 'page_3_sigle', 'page_4_sigle', 'page_0', 'page_1']\n",
      "['page_2', 'page_3', 'page_4', 'action_type_0_1_cnt', 'action_type_1_1_cnt']\n",
      "['action_type_2_1_cnt', 'action_type_3_1_cnt', 'action_type_4_1_cnt', 'action_type_5_1_cnt', 'action_type_0_3_cnt']\n",
      "['action_type_0_3_cnt_arg', 'action_type_1_3_cnt', 'action_type_1_3_cnt_arg', 'action_type_2_3_cnt', 'action_type_2_3_cnt_arg']\n",
      "['action_type_3_3_cnt', 'action_type_3_3_cnt_arg', 'action_type_4_3_cnt', 'action_type_4_3_cnt_arg', 'action_type_5_3_cnt']\n",
      "['action_type_5_3_cnt_arg', 'action_type_0_7_cnt', 'action_type_0_7_cnt_arg', 'action_type_1_7_cnt', 'action_type_1_7_cnt_arg']\n",
      "['action_type_2_7_cnt', 'action_type_2_7_cnt_arg', 'action_type_3_7_cnt', 'action_type_3_7_cnt_arg', 'action_type_4_7_cnt']\n",
      "['action_type_4_7_cnt_arg', 'action_type_5_7_cnt', 'action_type_5_7_cnt_arg', 'action_type_0_sigle', 'action_type_1_sigle']\n",
      "['action_type_2_sigle', 'action_type_3_sigle', 'action_type_4_sigle', 'action_type_5_sigle', 'action_type_0']\n",
      "['action_type_1', 'action_type_2', 'action_type_3', 'action_type_4', 'action_type_5']\n"
     ]
    }
   ],
   "source": [
    "# 列举所有的columns\n",
    "for i in range(1,int(len(data1_in.columns)/5)+1):\n",
    "    print (list(data1_in.columns)[(i-1)*5: i*5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 手动标定不需要的cols\n",
    "drop_cols = ['login_sum','login_max','login_var','login_mean','login_3_cnt','login_2_cnt','login_7_cnt','device_map','page_sum','action_type_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data1 = data1_in[[c for c in data1_in.columns if c not in drop_cols and c in select_cols]]\n",
    "# data2 = data2_in[[c for c in data2_in.columns if c not in drop_cols and c in select_cols]]\n",
    "# data3 = data3_in[[c for c in data3_in.columns if c not in drop_cols and c in select_cols]]\n",
    "\n",
    "data1 = data1_in[[c for c in data1_in.columns if c not in drop_cols]]\n",
    "data2 = data2_in[[c for c in data2_in.columns if c not in drop_cols]]\n",
    "data3 = data3_in[[c for c in data3_in.columns if c not in drop_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22342, 116)\n",
      "(37446, 116)\n",
      "(51709, 115)\n"
     ]
    }
   ],
   "source": [
    "print (data1.shape)\n",
    "print (data2.shape)\n",
    "print (data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 目前线上的参数，调过\n",
    "LGBM = lgb.LGBMClassifier(  max_depth=6,\n",
    "                            n_estimators = 280,\n",
    "                            learning_rate =0.05,     \n",
    "                            objective = 'binary',\n",
    "                            num_leaves=25,\n",
    "                            boosting_type = 'dart',\n",
    "                            feature_fraction=0.5,\n",
    "                            lambda_l1=1,\n",
    "                            lambda_l2=0.5,\n",
    "                            subsample=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 题目规定线下 F1计算方法\n",
    "def sroceF1(pred, real):\n",
    "    M = set(pred)\n",
    "    N = set(real)\n",
    "    Precision = len(M.intersection(N))/len(M)\n",
    "    Recall = len(M.intersection(N))/len(N)\n",
    "    F1 = 2*Precision*Recall/(Precision+Recall)\n",
    "\n",
    "    print(\"Precision=\",Precision,\"| Recall=\",Recall)\n",
    "    print(\"F1=\",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练模型，做预测\n",
    "def buildModelAndPredict(isOnLine=True, isTest=False, yuzhi=0.4, model=LGBM):\n",
    "    # 线上预测\n",
    "    # @return: 返回活跃用户\n",
    "    if (isOnLine):\n",
    "        train = pd.concat([data1, data2])\n",
    "        test = data3.copy()\n",
    "        train.pop('user_id')\n",
    "        label = train.pop('label')\n",
    "        \n",
    "        model.fit(train, label)\n",
    "        user_list = test.pop('user_id')\n",
    "        user_df = pd.DataFrame(user_list)\n",
    "        user_df['pre_act'] = model.predict_proba(test)[:,1]\n",
    "        return user_df\n",
    "\n",
    "    # 线下训练调试\n",
    "    else: \n",
    "        train = data1.copy()\n",
    "        test = data2.copy()\n",
    "        train.pop('user_id')\n",
    "        train_df_label = train.pop('label')\n",
    "        train_df = train\n",
    "        \n",
    "        real_user = test[test.label==1]['user_id']\n",
    "        user_list = test.pop('user_id')\n",
    "        test.pop('label')\n",
    "        test_df = test\n",
    "        \n",
    "        user_df = pd.DataFrame(user_list)\n",
    "        model.fit(train_df, train_df_label)\n",
    "        user_df['pre_act'] = model.predict_proba(test_df)[:,1]\n",
    "        \n",
    "        user_pre = user_df[user_df.pre_act>yuzhi]['user_id']\n",
    "        sroceF1(user_pre, real_user)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision= 0.7702635006512374 | Recall= 0.8437688635241178\n",
      "F1= 0.8053424119418621\n",
      "0.38\n",
      "Precision= 0.7710571227788374 | Recall= 0.842945727926247\n",
      "F1= 0.8054004456678464\n",
      "0.382\n",
      "Precision= 0.7724342834122269 | Recall= 0.8417384623827032\n",
      "F1= 0.8055985924739371\n",
      "0.384\n",
      "Precision= 0.7730621719822366 | Recall= 0.8406409482522087\n",
      "F1= 0.8054365256710219\n",
      "0.386\n",
      "Precision= 0.7737691646005161 | Recall= 0.8391593041760412\n",
      "F1= 0.8051387353235403\n",
      "0.388\n",
      "Precision= 0.7748236846110914 | Recall= 0.8380069143390221\n",
      "F1= 0.8051776863861647\n",
      "0.39\n",
      "Precision= 0.775489448258327 | Recall= 0.8368545245020029\n",
      "F1= 0.8050042229729729\n",
      "0.392\n",
      "Precision= 0.7761239677846875 | Recall= 0.8355375075454097\n",
      "F1= 0.8047356042387886\n",
      "0.394\n",
      "Precision= 0.7767168751595609 | Recall= 0.8347692476540636\n",
      "F1= 0.8046974185357597\n",
      "0.396\n",
      "Precision= 0.7774424552429667 | Recall= 0.8340558634692422\n",
      "F1= 0.8047547189791651\n",
      "0.398\n",
      "Precision= 0.7781251601660601 | Recall= 0.8331229764583219\n",
      "F1= 0.8046854295860496\n",
      "0.4\n",
      "Precision= 0.7789073731772438 | Recall= 0.8324644679800253\n",
      "F1= 0.8047958831799251\n",
      "0.402\n",
      "Precision= 0.7799043062200957 | Recall= 0.8318608352082533\n",
      "F1= 0.8050451407328731\n",
      "0.404\n",
      "Precision= 0.7807293547222366 | Recall= 0.8305986939581848\n",
      "F1= 0.8048923158734379\n",
      "0.406\n",
      "Precision= 0.7816264281652278 | Recall= 0.8296658069472644\n",
      "F1= 0.80492998988447\n",
      "0.408\n",
      "Precision= 0.7828521072002488 | Recall= 0.8287329199363441\n",
      "F1= 0.8051394146185424\n",
      "0.41\n",
      "Precision= 0.7837023178463777 | Recall= 0.8275256543928003\n",
      "F1= 0.8050180168156946\n",
      "0.412\n",
      "Precision= 0.7844127446897127 | Recall= 0.826812270207979\n",
      "F1= 0.805054633859635\n",
      "0.414\n",
      "Precision= 0.7853302009919081 | Recall= 0.8254952532513856\n",
      "F1= 0.8049119803092727\n",
      "0.416\n",
      "Precision= 0.7861105296211011 | Recall= 0.8242879877078417\n",
      "F1= 0.8047467252417562\n",
      "0.418\n",
      "Precision= 0.7867554530201343 | Recall= 0.8234099764034462\n",
      "F1= 0.8046655047593511\n",
      "0.42\n",
      "Precision= 0.7878166719226322 | Recall= 0.8225319650990507\n",
      "F1= 0.8048001288625198\n",
      "0.422\n",
      "Precision= 0.7885121617352848 | Recall= 0.821873456620754\n",
      "F1= 0.8048472472257302\n",
      "0.424\n",
      "Precision= 0.7895710059171598 | Recall= 0.8201174340119629\n",
      "F1= 0.8045543861538048\n",
      "0.426\n",
      "Precision= 0.7903183094115778 | Recall= 0.8188552927618943\n",
      "F1= 0.8043337645536869\n",
      "0.428\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.39, 0.43, 0.002):\n",
    "    user_df = buildModelAndPredict(isOnLine=False, isTest=False, yuzhi=i, model=LGBM)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df = buildModelAndPredict(isOnLine=True,isTest=False, yuzhi=0.4, model=LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_pre = user_df.sort_values(by='pre_act', axis = 0, ascending = False)['user_id'].head(24500)\n",
    "user_pre = user_df[user_df.pre_act>0.404]['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24700"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果数据提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_pre.to_csv('../Output/0624_114_8046854_24700_404.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// 复现82Line\n",
    "\n",
    "0624_114_8046854_24832_4    线上：0.819498\n",
    "\n",
    "0624_114_8046854_24700_404   线上: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
